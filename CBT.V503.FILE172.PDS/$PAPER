1
-
-
-
0                          3990-3 EXTENDED FUNCTIONS
-                              PAGING INTO CACHE
-
-
-
-
-
-
-                               David Cartwright
                              Sprecher Energie AG
                             CH-5036 OBERENTFELDEN
                                  Switzerland
1
-
-
-
0                                   ABSTRACT
+                                   ABSTRACT
- This paper describes the work done at the Sprecher Energie computer centre
  in Switzerland to make MVS/XA use the 3990-3 Extended Functions for paging
  operations in an environment without Expanded Storage. The problems of
  measuring the performance of the 3990-3 are described. A simple performance
  monitor program was written, showing that paging does not use the 3990
  Extended Functions. The changes to MVS/XA required to overcome this, and
  results obtained, form the rest of the text.
0 This paper is based on a presentation made at the European G.U.I.D.E. (GUIDE
  with the dots) Conference at Helsinki in June 1991.
-
-
-
-
-
-
-
-
-
-
-
-
-
                                     - ii -
1
-
-
0                                    PART 1
+                                    PART 1
0                                     TEXT
+                                     TEXT
-
  1.   COMPANY BACKGROUND
+ 1.   COMPANY BACKGROUND
+      _______ __________
0 Sprecher Energie AG is part of the Joint Venture between the French group
  Alsthom, and GEC of Great Britain. We use the renowned engineering skills
  the Swiss in the construction of high voltage switchgear, the sort of things
  to be seen outside power stations.
0 The computer centre at the company headquarters in Oberentfelden Switzerland
  houses an IBM 4381 model 24 running MVS/XA 2.2 and CICS 2.1.1 to drive
  online applications critical to production planning. Online response time is
  of paramount importance because the whole company revolves around these
  systems. We are always seeking new ways to improve response time. Budgetary
  constraints have prevented us from upgrading the machine - anyway, at that
  time where could we go from the top of the range 4381? Water cooling would
  be very expensive. That meant that Expanded Storage was not available
  and that MVS/ESA is at best a 'Long Term Consideration'.
-
  2.   3990-3 INSTALLATION
+ 2.   3990-3 INSTALLATION
+      ______ ____________
0 We installed a 3390-3 J03 (64Mb) in May 1989 after a study by IBM convinced
  us that it would give faster response times than the Solid State disks
  installed at that time. Indeed it did, the improvement was dramatic.
  Furthermore, it did not require constant tuning to ensure that the device
  was used to the optimum benefit; the box seemed to be self regulating. We
  installed DFP 2.4 in May 1990, one year later. We immediately enabled
  3990-3 Extended Functions, DASD Fast Write and Cache Fast Write.
  Unfortunately this upgrade did not bring us faster response times. What it
  did bring was some much needed relief in the Batch window - one overnight
  system came down from 12 hours to 7 hours. This was gratifying, but we
  really wanted to know why online response had not improved as much as we had
  hoped. We found it was very difficult to know what was going on.
-
-
-
-
0                                    - 1 -
1
-
- 3.   MONITORING CACHE
+ 3.   MONITORING CACHE
+      __________ _____
0 IBM had obtained for us a copy of a paper entitled 'IBM 3990 Extended
  Function' written by Daniel Asselin from the IBM ITSC in Santa Teresa. This
  is an excellent and comprehensive Beginners Guide to the box. In it are some
  very good notes on the IBM Cache RMF Reporter (CRR) program, 5798-DQD. This
  appeared at that time to be about the only product available which could give
  meaningful information about cache performance. The alternative was to use
  the LISTDATA functions of IDCAMS, but this only gives the total values of
  all the counters. To get useful information you have to do an awful lot of
  arithmetic. However, Asselin shows how to do this, and we tried it a couple
  of times. It was interesting, but we still did not know what it was telling
  us. It was a lot of effort to get not much information. We were looking for
  ways to get not much information for not much effort. We considered buying
  the CRR product from IBM, but it seemed to go for the overkill approach. We
  really were not terribly interested in Cache Write Hit Ratios, or the
  Sequential Read/Write Ratio or all these other technical terms. There was
  not much we could do about them anyway because there are no buttons to push
  on the box to tune it, all you can do is move data around. That is an
  extremely tedious business predicated on the User having a surplus of DASD
  space to play around with. The CRR product, cheap though it may be, did not
  seem to us to be a cost effective way of spending our limited budget. What
  we wanted was some way to get a quick overview of cache activity so that we
  could spot discrepancies without all that technical stuff, and it looked
  like we would have to Roll Our Own.
0 An obvious approach was to post-process an IDCAMS LISTDATA report.  While
  looking at this possibility we needed clarification of what exactly these
  counters were telling us, so we went to the definitive document, 'IBM 3990
  Storage Control Reference', GA32-0099. From this excellent manual we found
  the way that IDCAMS gets its data. On receipt of a valid channel command
  'READ SUBSYSTEM DATA', command code X'3E', the controller returns the
  statistics record for one or all the devices attached to the controller. The
  format of the statistics record for a device is shown in Figure 1. There is
  an awful lot of detailed data here.
0 Armed with this information it was fairly easy to write a program to read
  the statistics records for all devices directly from the 3990. It seemed too
  complicated to write it as an RMF User module like CRR, but the idea of
  having it synchronised with RMF seemed reasonable. We settled for a
  compromise of reporting on the same time boundaries as RMF, in our case 15
  minutes.  It was obviously a good idea to write the statistics record as
  part of a User SMF record for post processing, so we copied CRR in that as
  well. However, we also wanted real time information, so we added a
  multi-line WTO to show the Efficiency of cache for each device. Asselin
  illustrates the efficiency of the 3990 by calculating the percentage of
  physical I/O's to requests. We defined Efficiency as 100 minus that figure,
  in other words the percentage of I/O's dealt with entirely in cache. A
  typical report can be seen in Figure 2. A matrix of 64 possible devices are
  presented in rows of eights, with a dual heading. The User has to be careful
  that he is looking at the device number he thinks he is, but it gives neat
  and compact display.
0 This program gave immediate benefits. We could see devices which seemed to be
  candidates for cache, as well as those which are not. The most efficient use
-                                    - 2 -
1
-
- of cache turns out to be WORK disks - the reason for that is that the major
  user of work space is SORT, and DF/SORT uses Cache Fast Write, so very few
  requests result in physical I/O. Another surprise was that SYSRES also made
  good use of cache. We use the Legent product QUICK-FETCH to buffer linklist
  so did not expect much activity on SYSRES. What we forgot were the ISPF
  panel libraries, CLIST libraries, CSP files and other non-load library
  activity. The biggest shock was that the Efficiency of cache for page
  volumes was zero - they did not use cache at all. It did not seem a good
  idea to hang page volumes under cache if they were never going to use it, so
  we reconfigured the machine to move those page volumes off to a 3880 and
  replaced them with TEST disks. Nobody cared about test response time, but at
  least they would use cache if it was available. By getting as much User data
  as possible under cache we implimented a long standing IBM recommendation -
  have a dedicated string for paging. Let me tell you, it does work. Response
  times improved - not dramatically, but it made us feel good.
0 This was all well and good, but the display from our Simple Software Cache
  Reporter lacked something. It was no good worrying about a disk with low
  cache Efficiency if it only had one or two I/O's in a quarter hour.
  Similarly a high percentage of a low number of I/O's is still only a small
  benefit. What was needed was some idea of the number of I/O's per device, so
  we added to the display the percentage of the total I/O's to the 3990
  controller which were aimed at each volume. Figure 3 shows the result. The
  display has since been slightly enhanced at the suggestion of ATAG
  Informatik to include read, write and total I/O counts. We never have done
  any post processing of the SMF records, but they are available if we get
  deeply involved in Read/Write ratios or other advanced calculations.
-
  4.   PAGING WITH CACHE
+ 4.   PAGING WITH CACHE
+      ______ ____ _____
0 We were intrigued as to why IBM did not make MVS/XA use cache for paging, it
  seemed such a good application for 3990-3 Extended Functions which could
  guarantee the integrity of the data with greatly reduced I/O time. Surely it
  was wrong to suspect that the only reasons could be commercial ones to do
  with the sales of main or expanded storage? We asked our SE, who came back
  with the reply that the application did not suit the cache management
  algorithms. We were so impressed with the benefits we got from the 3990 in
  commercial applications that we thought it might be worth trying anyway. We
  had already seen that the Cache Efficiency of page volumes was zero, so we
  looked into the reasons for this. They were not hard to find, a GTF trace of
  channel programs to a page volume showed that they all began with a Define
  Extent command, hex code '63', specifying BYPASS CACHE. We wanted to change
  that Define Extent. Logically we should just zap off the BYPASS CACHE bit,
  the default for the 3990 is to use DASD Fast Write. However, we were not
  that fussed about the integrity of paging - if the cache took a hit we could
  allow some System 028 abends, or even an IPL, provided it did not happen too
  frequently. Our 3990 has not been as reliable as we would like, but we had
  never lost any data because of its problems and we thought it might be a
  reasonable risk. The gain would be that we could use Cache Fast Write,
  offloading the Non-Volatile Storage and possibly giving faster response
  times.
-
0                                    - 3 -
1
-
- This turns out to be more complex than we thought. It is not just a question
  of zapping a bit in the Define Extent. Every Channel Program using Cache
  Fast Write has to supply a Cache Identifier which matches that currently in
  use by the box. If cache is re-initialized, maybe after a failure, this
  identifier is incremented, and Channel Programs with the old Identifier are
  terminated with a Unit Check. Therefore we could not just zap MVS modules
  and re-IPL, because the next time we powered the box off our paging
  sub-system would be inoperative. The CCW's have to be changed dynamically
  using the current Cache Fast Write Identifier. This should not change over
  the life of an IPL, but certainly will change over a long period. It would
  be enough to zap the modules in storage once per IPL. We had already written
  a monitor program to read data from the 3990, so it was not difficul to
  modify that code to read the Cache Fast Write Identifier. All we had to do
  now was to find out what to do with it.
0 From the System Logic Library for MVS/XA we were able to deduce that of the
  the modules involved may be ILRCPBLD. The name was hopeful, anyway. If you
  browse through that module you will find near the end the hex string
  X'00C410000000'. These are the Define Extent Attribute Bytes specifying
  BYPASS CACHE. We modified our 3990 interrogation program to look up the
  address of ILRCPBLD in the Nucleus Map Table, then go search the module for
  this hex string. When found, the program overwrites it with X'00C21000'
  followed by the two byte Cache Fast Write Identifier read from the box. This
  says 'Use Cache Fast Write, and here is the Identifier'. As the module is
  Page Protected you have to be pretty tricky to modify it, but IBM in its
  infinite wisdom provides a way to do this if you look hard enough. Naturally
  we also wrote a version of the program to restore the IBM default so that we
  could do planned maintenance on cache. Another version zaps in Attribute
  Bytes X'00C010000000', which allows DASD Fast Write. This could probably be
  permanently zapped in without risk.
0 Came the great day (or rather night, we are not that foolhardy). ZAPPO!
  Well, at least the machine was still running, so we seemed to have got
  something right. After a fifteen minute wait out came the Cache Statistics.
  They were extremely disappointing. The cached disk with a Page Dataset
  defined on it had only 1% of I/O's and Cache Efficiency was less than 5%
  This was not going to deliver the instant response we had hoped for, could
  IBM just possibly have been right? Another GTF trace showed that only some
  Channel Programs had the new Define Extent Attributes, so although they were
  working the effects would be entirely negated by the BYPASS CACHE commands
  causing destaging before reading the data back. Another look through the
+                   ______
  logic manual and we added a zap of ILRSWAP to the program. This was better,
  but we still had some Channel Programs with BYPASS CACHE, which would be
  seriously damaging to Cache Efficiency. ILRPGFLT was finally identified as
  the culprit - why have one module build Channel Programs when IBM can do it
  in three?  Anyway, at last we were paging directly into cache.
0 Yes Folks, IBM was right after all. The Efficiency was dreadful, rarely
  hitting 20%. A big surprise was that paging only accounts for 1 or 2 percent
  of all I/O's. We thought this may be why the cache management algorithms
  make such a bad job of it. Response times were no better, possibly they were
  worse because they were more variable depending on other activity on the
  machine. It looked like a dedicated string for paging was the optimum
  architecture. It was only when we attended Bill Thompson's presentation on
  Non-Synchronous DASD at the European G.U.I.D.E. Conference in Wiesbaden that
-                                    - 4 -
1
-
- we found out that the real reason that paging is not an efficient cache
  application is because MVS uses Head Switching channel programs to optimise
  slot placement.
-
  5.   SWAPPING
+ 5.   SWAPPING
+      ________
0 We were deeply unhappy that it had ended that way, but could not see what to
  do about it. Thinking about it, we figured maybe the problem was because we
  allowed the Auxiliary Storage Manager to mix Paging and Swapping on the same
  page datasets. Maybe this was causing poor cache management by swamping
  Demand Paging with Swap Sets and vice versa. We thought that the best
  candidate of the two would be Swapping because that generates large volumes
  of contiguous data - well suited to cache management. We tried this, and the
  results were very encouraging. Cache Efficiency for a Swap Dataset seemed to
  be between 20 and 50 percent, although the amount of I/O was small. It would
  probably be beneficial to us to separate paging and swapping anyway because
  of our mix of CICS and TSO. By using Cache Fast Write for swapping we could
  allocate Swap Datasets to ordinary, non-dedicated volumes. We may get
  a lot of destaging by the cache management algorithms, but this is happening
  asynchronously with the swapping I/O, so is not a problem. Swap Sets would
  be promoted into cache as soon as referenced anyway, so we get some benefit
  from cache reads as well as the speed advantage of writing directly into
  cache. Because we can use non-dedicated disks we could allocate enough Swap
  Datasets to maximise overlapping of swap I/O, in our case we defined ten
  Swap Datasets of ten cylinders each. As we had now moved all our TEST data
  under cache we were able to use those volumes. Although the figures showed
  that this was not using cache terribly efficiently, the gain in actual
  response times made it worthwhile to do this, and we ran like this for a
  long time. We concluded that response times had improved from studying RMF
  and NETSPY graphs. We therefore tuned the system to take advantage of these
  new characteristics, for example by lowering UIC thresholds and by reducing
  Logical Swap Think Time. These measures were intended to maximise the
  utilisation of real storage by using the faster swap capability of the
  3990-3. In turn they increased the Cache Efficiency for the Swap Datasets to
  between 50 and 80 percent. Before examining the performance of cache for
  swapping more closely, we should consider the System Availability problems.
-
-
-
-
-
-
0                                    - 5 -
1
-
- 6.   SYSTEM AVAILABILITY
+ 6.   SYSTEM AVAILABILITY
+      ______ ____________
0 As explained, we felt able to risk a few problems in return for better
  performance,but it was not long before we found out what we had let
  ourselves in for. Figure 4 shows a the sort of thing we began to find on
  SYSLOG. Message IEA459I seemed so innocuous, but was a harbinger of doom.
  The first time this happened was during Prime Shift, so everybody paniced.
  CICS was still running, having been defined as non-swappable, but every
  else seemed to have crashed. We therefore did an IPL over the lunch break
  when not many people would notice (the whole of Switzerland closes for lunch
  - even some restaurants). The next time it happened was during the evening
  so guess what happened. Our Operators (God bless 'em) just kept restarting
  all those darned Initiators. After every Swap Set on every Swap Dataset had
  been marked unusable, MVS/XA finally admitted defeat and went back to using
  only Local Page Datasets.  The system still ran fine.
0                            GORILLAS - 1, MVS - 0
0 This scenario occurred several times, mostly overnight. It was annoying, but
  not fatal. Of course, we wanted to schedule an IPL as soon as possible after
  that to be able to swap into cache again, but it could take as long as three
  weeks before the schedule allowed an IPL. In order to reduce the time
  to recover from the dreaded IEA459I we shrank the Swap Datasets to five
  cylinders each, and eventually we wrote an MPF exit  to disable our changes
  as soon as this message was detected. Hopefully that way only one or two
  address spaces would get killed. This was going a bit far from our Keep It
  Simple philosophy, but all of the code was lying around anyway and just
  needed to be stitched together in a new way.
0 Having done all this work to protect ourselves from IEA459I, what happened?
  IBM did an EC change on the microcode and we began to get failures without
  IEA459I as shown in Figure 5. We tried to APAR this,saying that if cache was
  re-initiated it should issue the message to say it has been re-initiated,
  but IBM came back and said that IEA459I was driven by Sense Bytes and if it
  did not get the right combination of bits it would not issue IEA459I.
0 It's Software - no, it's Hardware. Ho Hum.
-
  7.   CACHE SWAP PERFORMANCE
+ 7.   CACHE SWAP PERFORMANCE
+      _____ ____ ___________
0 Although we could satisfy ourselves that cache swapping had improved
  response times, this was based more on the hands-on feel of the system
  on some graphs which were open to various interpretations. For a wider
  audience we needed better hard numbers.
0 There is a chapter in the MVS/XA Initialisation and Tuning Guide
  which deals with 'The Use of GTF to Track Sysevents'. At the end of that
  chapter are some illustrations of the sort of reports which may be produced
  including an example of the 'Distribution of Time Needed to Complete a
  Swapout'. This looked like exactly what we wanted - we could run such a
  report against various configurations to compare them. We asked IBM if the
  program to produce this report was available. IBM were as helpful as ever,
  so we had to do it ourselves. Fortunately we had a copy of a Sysevent
-                                    - 6 -
1
-
- Analysis System written many years ago by Standard Oil of Indiana and
  contributed to the Public Domain. Our grateful thanks to them for their
  public spirit. These programs were fairly easy to modify for MVS/XA and
  provided the foundation for our measurements, fitting in nicely with our
  'Keep It Simple' philosophy and yet being accurate to within one microsecond
  - if you are like us and confused between micro- and milli-, that is ten to
  the minus six, one millionth. This is the Timer resolution on a 4381.
0 We quickly realised that a much more interesting measure was the
  Distribution of Time Needed to Complete a Swap-IN, because the concept of
  Logical Swap means that Swap-Out times do not actually tell you much. A
  TSO User may or may not get swapped out to disk, but what determines his
  perception of response time is how long it takes to get him swapped back IN.
  Hopefully with a Logical Swap this will not be a long time, but if he is
  physically swapped then the performance of the swap mechanism will become
  important. We therefore measured the time between the SWINSTAT Sysevent
  code 10 sub-type 1  - page frames acquired - and sub-type 2, Swap-in
  complete. This should be a measure of the I/O time for a physical Swap-in.
  Figure 6 shows the distribution of Swap-In times for various configurations.
  The IBM recommendation of using only Local Page Datasets on a dedicated
  string gives the worst performance. For our combination of TSO, Batch and
  CICS the addition of Swap Datasets on non-dedicated disks moves the average
  I/O time down about 30 milliseconds. If we add cache the improvement is
  dramatic, many more swaps completing much faster. The difference between
  Cache Fast Write and DASD Fast write is nothing compared to the overall
  improvement, indeed DASD Fast Write provides slightly better Swap-in
  performance. This may be because data takes slightly longer to process in
  cache, so the chance of a Read Hit is higher.
0 We still have some interest in Swap-out times however, because faster
  Swapouts will free up real storage frames that much quicker. If we look at
  our version of the Distribution of Time Needed To Complete a Swapout shown
  in Figure 7 we can again see that cache gives tremendous benefit. The
  difference between Local versus Swap Datasets is less clear, and this time
  Cache Fast Write is faster than DASD Fast Write, which is what we expected
  in the first place. However, the difference in performance between the
  cache operations is not as great as we first thought. Indeed, one can make a
  good case for DASD Fast Write giving the better overall gain with much less
  risk, so we have now changed to using only DASD Fast Write for Swap
  Datasets.
0 These figures agree with our perception that the system is much more
  responsive. TSO response time is better and Batch throughput is higher. The
  SRM copes with transient overloads much more effectively as the swapping
  sub-system is speeded up. We can now encourage swapping because it does not
  represent a bottleneck in our system, which can happily run at average
  paging rates of 200 per second with instantaneous peaks up to 450 pages per
  second, or higher. Remember, this is a 4381, 8 MIPS with no expanded
  storage. Using the Sysevent Analysis System to analyze Think Time, we found
  that setting the Logical Think Time threshold to 4 seconds, plus 2 seconds
  grace from MVS, made 80 percent of Swaps into Logical Swaps. Extending the
  Logical Think Time beyond this value was not cost effective, so that's what
  we used. This in turn meant that Started Tasks were no longer eligible for
  Logical Swap, so we had to make a couple of them non-swappable, but our
  storage frame availability improved dramatically. CICS response time also
-                                    - 7 -
1
-
- improved because fewer pages were being stolen, and the MVS overhead went
  down because of less demand paging.  With less overhead TSO response times
  improved, even though we were now doing much more swapping.
-
  8.   MVS/ESA
+ 8.   MVS/ESA
+      _______
0 Users of MVS/ESA will have to judge for themselves the applicability of this
  work to their environment. It seems reasonable to assume that the same
  modules perform the same functions in MVS/ESA, so the code changes will
  probably work. The ability to dynamically remove page datasets may make the
  option of using Cache Fast Write more interesting, or else you could
  probably get the same benefits by doing a SYSMOD to permanently zap out the
  BYPASS CACHE bit in ILRSWAP. Whether it will be more cost effective to
  perform a two stage Swapout via expanded storage, or a single stage Swap
  to 3990-3 cache is an interesting exercise that we will have to leave to
  someone else, because it seems likely that we will downsize to VSE/ESA
-
-
-
-
-
-
-
-
-
-
-
-
-
                                     - 8 -
1
-
-
-
0                                    PART 2
+                                    PART 2
0                                   FIGURES
+                                   FIGURES
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
0                                    - 9 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |  *******************************************************************
  |  *        IBM 3990-3 SUBSYSTEM STATISTICS RECORD
  |  *******************************************************************
  |           SPACE 2
  |  SSREC    DSECT ,              OFF. SUBSYSTEM STATISTICS RECORD
  |  SSFLAG1  DS    XL1             0   FLAG BYTE
  |  SSFALSE  EQU   B'10000000'         WHEN SET CACHE NOT AVAILABLE
  |  SSDEVADD DS    XL1             1   DEVICE UNIT ADDRESs
  |  SSSENSE1 DS    0XL2            2   DEVICE STATUS SENSE BYTES GROUP1
  |  SSSENSEA DS    XL1             2   DEVICE STATUS BYTE 26
  |  SSS1ACX  EQU   B'11000000'         CACHING DEACTIVATED
  |  SSS1A8X  EQU   B'10000000'         CACHING DEACTIVATE PENDING
  |  SSS1A3X  EQU   B'00110000'         DASD FAST WRITE DEACTIVATED
  |  SSS1A2X  EQU   B'00100000'         DASD FAST WRITE DEACTIVATE PENDI
  |  SSS1AX8  EQU   B'00001000'         DEVICE IS DUPLEX PRIMARY
  |  SSS1AX4  EQU   B'00000100'         DEVICE IS DUPLEX SECONDARY
  |  SSSENSEB DS    XL1             3   DEVICE STATUS BYTE 27 - DUPLEX
  |  SSS1BCX  EQU   B'11000000'         PINNED DATA - FAST WRITE SUSPEND
  |  SSS1B4X  EQU   B'01000000'         PINNED DATA - FAST WRITE NOT SUS
  |           SPACE 1
  |  SSIORDRQ DS    XL4             4   NO. READ NORMAL I/O REQUESTS
  |  SSIORDHT DS    XL4             8   NO. READ NORMAL HITS
  |  SSIOWTRQ DS    XL4            12   NO. WRITE NORMAL I/O REQUESTS
  |  SSIOFWHT DS    XL4            16   NO. DASD FAST WRITE I/O HITS
  |  SSIORSRQ DS    XL4            20   NO. READ SEQUENTIAL I/O REQUESTS
  |  SSIORSHT DS    XL4            24   NO. READ SEQUENTIAL HITS
  |  SSIOWSRQ DS    XL4            28   NO. WRITE SEQUENTIAL I/O REQUEST
  |  SSIOWSHT DS    XL4            32   NO. WRITE SEQUENTIAL HITS
  |  SSRDCWRQ DS    XL4            36   NO. CACHE WRITE SEARCH OR READ R
  |  SSRDCWHT DS    XL4            40   NO. CACHE WRITE SEARCH OR READ H
  |  SSIOCWRQ DS    XL4            44   NO. CACHE WRITE I/O REQUESTS
  |  SSIOCWHT DS    XL4            48   NO. CACHE WRITE I/O HITS
  |  SSICLRQ  DS    XL4            52   NO. INHIBIT CACHE LOADING REQUES
  |  SSBYCRQ  DS    XL4            56   NO. BYPASS CACHE REQUESTS
  |  SSSQDCTR DS    XL4            60   NO. SEQUENTIAL DASD-CACHE TRACK
  |  SSDCTR   DS    XL4            64   NO. DASD-CACHE TRACK XFER
  |  SSCDTR   DS    XL4            68   NO. CACHE-DASD TRACK XFER
  |  SSDFFAIL DS    XL4            72   NO. DASD FAST WRITE NVS CONSTRAI
  |  SSDFNW   DS    XL4            76   NO. DASD FAST WRITE NORMAL WRITE
  |  SSDFSQ   DS    XL4            80   NO. DASD FAST WRITE SEQUENTIAL W
  |           DS    XL4            84   RESERVED. SET TO ZEROES
  |  SSSQSQ   DS    XL4            88   NO. SEQUENTIAL-DETECTED READ SEQ
  |  SSSENSE2 DS    0XL1           92   DEVICE STATUS SENSE BYTES GROUP1
  |  SSSENSEC DS    XL1            92   DEVICE STATUS BYTE 36 - NVS
  |  SSS2C4X  EQU   B'01000000'         DATA EXISTS IN FAILED NVS
  |           DS    XL1            93   RESERVED. SET TO ZEROES
  |  SSSSID   DS    XL2            94   SUBSYSTEM IDENTIFIER (SSID)
  |           SPACE 2
  |  SSLENTH  EQU   *-SSREC             LENGTH OF DEVICE ENTRY
  |
  |
  |                   Figure 1:  STATISTICS RECORD FORMAT
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
0                                    - 10 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |             SSCRI001   CACHE EFFICIENCY % 522
  |             XX0   0    1    2    3    4    5    6    7
  |             XX7   8    9    A    B    C    D    E    F
  |             240 43.4  0.0 24.5 87.4 55.3 94.4 51.4  0.0
  |             247 22.8 22.5 72.7 26.2  0.0 68.7 23.7  0.0
  |             250 96.0 75.4 76.3 43.8 49.4 45.0 72.5 54.9
  |             257 21.6 22.3 40.0 18.2  0.0  0.0  0.0  0.0
  |             260  0.0 92.4 62.3  0.0 91.3 85.5 44.2 27.2
  |             267  0.0 74.1  0.0 66.4   -    -    -    -
  |             270 15.6 49.1 68.3  0.0   -    -    -    -
  |             277   -    -    -    -    -    -    -    -
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |                    Figure 2:  CACHE EFFICIENCY REPORT
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
-
0                                    - 11 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |             SSCRI001   CACHE % USE/GAIN 361
  |             XX0   0     1     2     3     4     5     6     7
  |             XX8   8     9     A     B     C     D     E     F
  |             240 01/24 00/00 00/19 02/72 00/54 06/95 01/32 00/00
  |             248 01/00 00/28 01/81 01/27 01/76 00/66 01/19 00/04
  |             250 06/97 12/77 00/33 00/31 01/67 05/44 08/06 08/53
  |             258 01/22 00/18 02/54 00/00 01/05 00/00 00/00 00/00
  |             260 00/26 01/26 01/00 00/70 09/94 14/87 01/47 05/22
  |             268 00/28 03/61 00/61 05/72   -     -     -     -
  |             270 00/23 00/09 03/32 00/27   -     -     -     -
  |             278   -     -     -     -     -     -     -     -
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |              Figure 3:  CACHE EFFICIENCY REPORT - VERSION 2
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
-
                                     - 12 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |   IEA459I CACHING REINITIATED - 242
  |   IOS000I 242,1A,CMD,63,0E00,80000022C27604F6,00760002,TST004,*MASTE
  |   IOS000I 240,07,CMD,63,0E00,80000022C09D95F6,01E5000A,TEST01,*MASTE
  |   IOS000I 261,07,CMD,63,0E00,8000002621C740F6,01300002,TST001,*MASTE
  |   $HASP310 INIT     TERMINATED AT END OF MEMORY
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |                        Figure 4:  MESSAGE IEA459I
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
0                                    - 13 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |   IOS071I 267,07,S301T813, MISSING CHANNEL AND DEVICE END
  |   IEC705I TAPE ON 381,504395,SL,NOCOMP,S301T810,SAVE,SAVE.P000CICS.C
  |   G0944V00
  |   IEA989I SLIP TRAP ID=X028 MATCHED
  |   IEA989I SLIP TRAP ID=X028 MATCHED
  |   IOS000I 242,1A,CMD,63,0E00,80000022C27604F6,00760002,TST004,*MASTE
  |   IOS000I 240,07,CMD,63,0E00,80000022C09D95F6,01E5000A,TEST01,*MASTE
  |   IOS000I 261,07,CMD,63,0E00,8000002621C740F6,01300002,TST001,*MASTE
  |   $HASP310 INIT     TERMINATED AT END OF MEMORY
  |   $HASP310 INIT     TERMINATED AT END OF MEMORY
  |   IOS000I 25A,0A,CMD,63,0E00,80000025CA4A4EF6,053D0001,TSO001,*MASTE
  |   IOS000I 240,17,CMD,63,0E00,80000023C09D95F6,01E50001,TEST01,*MASTE
  |   IOS000I 246,1A,CMD,63,0E00,80000022C66A02F6,006A0002,TEST03,*MASTE
  |   IOS000I 261,17,CMD,63,0E00,80000026E1C740F6,01300001,TST001,*MASTE
  |   $HASP311 P000T905 RE-QUEUED AT END OF MEMORY
  |   IOS000I 248,17,CMD,63,0E00,80000023C82012F6,01200004,TST006,*MASTE
  |   IOS000I 25A,1A,CMD,63,0E00,80000024CA4A4EF6,053D0000,TSO001,*MASTE
  |   IOS000I 242,1A,CMD,63,0E00,80000022C27604F6,00760000,TST004,*MASTE
  |   $HASP311 P000T920 RE-QUEUED AT END OF MEMORY
  |   $HASP314    INIT 23 DRAINED  ******** C=PANLTEI
  |   $HASP314    INIT 24 DRAINED  ******** C=PANLTEI
  |   IEF402I INIT     FAILED IN ADDRESS SPACE 0016 865
  |           SYSTEM ABEND S028 - REASON CODE 85000402
  |   IEF402I INIT     FAILED IN ADDRESS SPACE 0039 867
  |           SYSTEM ABEND S028 - REASON CODE 85000402
  |   IEF402I P000T905 FAILED IN ADDRESS SPACE 0039 869
  |           SYSTEM ABEND S028 - REASON CODE 85000402
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |
  |                           Figure 5:  I/O ERROR
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
-
0                                    - 14 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |  40   +          @
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |          *
  |       |
  |       |
  |       |
  |  30   +
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |  20   +                  *
  |       |
  |       |
  |       |                                          X
  |       |              *               #
  |       |
  |       |              @
  |       |
  |       |                          #                   X
  |       |                                  #
  |  10   +                                      X
  |       |                      #                           X
  |       |                                      #   #
  |       |                          *       X
  |       |                  #           X                       X
  |       |                      *   @                               X
  |       |  *                           *               #   #
  |       |                          X                           #   #
  |       |  @   *       #       X           *   @
  |       |      #   #       X                   *   *   *   @   *   *
  |   0   +-#---X---X---X-----+---------+---------+---------*---------+-
  |      25        50        75        100       125       150       175
  |
  |           X PLOTS LOCAL PAGE DATASETS ONLY
  |           #       SWAP DATASETS WITH NO CACHE
  |           @       SWAP DATSETS WITH DASD FAST WRITE
  |           *       SWAP DATSETS WITH CACHE FAST WRITE
  |
  |          X-AXIS REPRESENTS MILLISECONDS, Y-AXIS REPRESENTS  PERCENT
  |
  |
  |                 Figure 6:  SWAP IN TIMES (milliseconds)
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
0                                    - 15 -
1
-
- Ð×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
  |
  |  40   +
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |  30   +
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |  *
  |       |
  |       |
  |  20   +
  |       |  @
  |       |
  |       |
  |       |
  |       |
  |       |
  |       |          *   *
  |       |              @
  |       |                          X
  |  10   +
  |       |                      X
  |       |                  X                                   X
  |       |          @   #   #   @   *                       #
  |       |              X       #       @       #   #   #           #
  |       |                  *           #   #       *       X
  |       |      @           @       @           @   @           @   @
  |       |                                  @   *   X               X
  |       |                          #                       @   *
  |       |                                              @   *
  |   0   +-#---X---X---------+---------+---------+---------+-------*-+-
  |      25        50        75        100       125       150       175
  |
  |           X PLOTS LOCAL PAGE DATASETS ONLY
  |           #       SWAP DATASETS WITH NO CACHE
  |           @       SWAP DATSETS WITH DASD FAST WRITE
  |           *       SWAP DATSETS WITH CACHE FAST WRITE
  |
  |          X-AXIS REPRESENTS MILLISECONDS, Y-AXIS REPRESENTS  PERCENT
  |
  |
  |                 Figure 7:  SWAP OUT TIMES (milliseconds)
  |
  ¿×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
0                                    - 16 -
1
-
-                               LIST OF FIGURES
+                               LIST OF FIGURES
-
  Figure
+ ______
+
+
0 1.  STATISTICS RECORD FORMAT  . . . . . . . . . . . . . . . . . . . .10
0 2.  CACHE EFFICIENCY REPORT . . . . . . . . . . . . . . . . . . . . .11
0 3.  CACHE EFFICIENCY REPORT - VERSION 2 . . . . . . . . . . . . . . .12
0 4.  MESSAGE IEA459I . . . . . . . . . . . . . . . . . . . . . . . . .13
0 5.  I/O ERROR . . . . . . . . . . . . . . . . . . . . . . . . . . . .14
0 6.  SWAP IN TIMES (milliseconds)  . . . . . . . . . . . . . . . . . .15
0 7.  SWAP OUT TIMES (milliseconds) . . . . . . . . . . . . . . . . . .16
-
-
-
-
-
-
-
-
-
-
-
-
-                                    - 17 -
